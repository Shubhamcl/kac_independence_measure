@article{Feuerverger,
 ISSN = {03067734, 17515823},
 URL = {http://www.jstor.org/stable/1403753},
 abstract = {A new and consistent rank test for bivariate dependence is developed. Let Xi ′ and Yi ′ denote the (approximate) normal scores associated with the iid vectors (Xi,Yi),i=1,... ,n. Then the proposed test statistic may be obtained by removing the first Hájek projection from the quantity $\xi \equiv n^{-2}\mathop{\sum\sum}|X_{j}^{\prime}-X_{k}^{\prime}|\cdot |Y_{j}^{\prime}-Y_{k}^{\prime}|$. Empirical characteristic function considerations are used in our development and some related graphical methods are proposed. Some difficulties that arise in extensions to dimension k > 2 are noted. A small simulation study provides evidence of the effectiveness of the new procedure. /// Un nouveau test du rank convergent pour la dépendance bivariée est exposé dans cet article. Soit Xi ′ et Yi ′ les scores normaux (approchés) associés aux vecteurs iid (Xi,Yi),i=1,... ,n. La statistique du test proposée peut alors être obtenue en enlevant la première projection de Hájek de la quantité $\xi =n^{-2}\mathop{\sum\sum}|X_{j}^{\prime}-X_{k}^{\prime}|\cdot |Y_{i}^{\prime}-Y_{k}^{\prime}|$. Des considérations liées aux fonctions caractéristiques empiriques sont utilisées dans notre développement et des méthodes graphiques correspondantes sont proposées. Des difficultés apparaissant lors d'extensions aux dimensions k > 2 sont indiquées. Une étude de simulation atteste de l'efficacité de la nouvelle méthode.},
 author = {Andrey Feuerverger},
 journal = {International Statistical Review / Revue Internationale de Statistique},
 number = {3},
 pages = {419--433},
 publisher = {[Wiley, International Statistical Institute (ISI)]},
 title = "{A Consistent Test for Bivariate Dependence}",
 volume = {61},
 year = {1993}
}

@article{KacTheorem,
author = {David Applebaum, B.V. Rajarama Bhat, Johan Kustermans, J. Martin Lindsay, Michael Schuermann, Uwe Franz},
title = "{Quantum Independent Increment Processes I: From Classical Probability to Quantum Stochastic Calculus}",
year={2005},
publisher={Springer}
}

@article{CHAUDHURI201915,
	title = "{A fast algorithm for computing distance correlation}",
	journal = {Computational Statistics and Data Analysis},
	volume = {135},
	pages = {15-24},
	year = {2019},
	issn = {0167-9473},
	doi = {https://doi.org/10.1016/j.csda.2019.01.016},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947319300313},
	author = {Arin Chaudhuri and Wenhao Hu},
	keywords = {Distance correlation, Dependency measure, Fast algorithm, Merge sort}
}

@unknown{Bottcher,
author = {Böttcher, Björn and Keller-Ressel, Martin and Schilling, René},
year = {2018},
month = {10},
pages = {},
title = "{Distance multivariance: New dependence measures for random vectors}"
}

@inproceedings{Belu2012MultivariateMO,
  title="{Multivariate measures of dependence for random variables and Levy processes}",
  author={Alexandru Belu},
  year={2012}
}

@article{Szekely,
author = {Gábor J. Székely and Maria L. Rizzo and Nail K. Bakirov},
title = "{Measuring and testing dependence by correlation of distances}",
volume = {35},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2769 -- 2794},
keywords = {Distance correlation, distance covariance, multivariate independence},
year = {2007},
doi = {10.1214/009053607000000505},
URL = {https://doi.org/10.1214/009053607000000505}
}

@inproceedings{Gretton2005MeasuringSD,
  title="{Measuring Statistical Dependence with Hilbert-Schmidt Norms}",
  author={Arthur Gretton and Olivier Bousquet and Alex Smola and Bernhard Sch{\"o}lkopf},
  booktitle={ALT},
  year={2005}
}

@book{Cover2006,
  added-at = {2009-04-20T21:27:16.000+0200},
  at = {2008-03-31 06:17:47},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  biburl = {https://www.bibsonomy.org/bibtex/22e9bfa879286689a14feb55b69d326c1/ywhuang},
  howpublished = {Hardcover},
  id = {1877660},
  interhash = {87ae368776946bf7a71ee476e81a2191},
  intrahash = {2e9bfa879286689a14feb55b69d326c1},
  isbn = {0471241954},
  keywords = {information-theory book},
  month = {July},
  priority = {0},
  publisher = {Wiley-Interscience},
  timestamp = {2009-04-20T21:27:16.000+0200},
  title = "{Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)}",
  year = 2006
}


@Article{e20110813,
AUTHOR = {Amigó, José M. and Balogh, Sámuel G. and Hernández, Sergio},
TITLE = "{A Brief Review of Generalized Entropies}",
JOURNAL = {Entropy},
VOLUME = {20},
YEAR = {2018},
NUMBER = {11},
ARTICLE-NUMBER = {813},
URL = {https://www.mdpi.com/1099-4300/20/11/813},
ISSN = {1099-4300},
ABSTRACT = {Entropy appears in many contexts (thermodynamics, statistical mechanics, information theory, measure-preserving dynamical systems, topological dynamics, etc.) as a measure of different properties (energy that cannot produce work, disorder, uncertainty, randomness, complexity, etc.). In this review, we focus on the so-called generalized entropies, which from a mathematical point of view are nonnegative functions defined on probability distributions that satisfy the first three Shannon&ndash;Khinchin axioms: continuity, maximality and expansibility. While these three axioms are expected to be satisfied by all macroscopic physical systems, the fourth axiom (separability or strong additivity) is in general violated by non-ergodic systems with long range forces, this having been the main reason for exploring weaker axiomatic settings. Currently, non-additive generalized entropies are being used also to study new phenomena in complex dynamics (multifractality), quantum systems (entanglement), soft sciences, and more. Besides going through the axiomatic framework, we review the characterization of generalized entropies via two scaling exponents introduced by Hanel and Thurner. In turn, the first of these exponents is related to the diffusion scaling exponent of diffusion processes, as we also discuss. Applications are addressed as the description of the main generalized entropies advances.},
DOI = {10.3390/e20110813}
}

@article{Pczos2012CopulabasedKD,
  title="{Copula-based Kernel Dependency Measures}",
  author={Barnab{\'a}s P{\'o}czos and Zoubin Ghahramani and Jeff G. Schneider},
  journal={ArXiv},
  year={2012},
  volume={abs/1206.4682}
}

@article{Szkely2013PartialDC,
  title="{Partial Distance Correlation with Methods for Dissimilarities}",
  author={G{\'a}bor J. Sz{\'e}kely and Maria L. Rizzo},
  journal={arXiv: Methodology},
  year={2013}
}

@inproceedings{Loshchilov2019DecoupledWD,
  title="{Decoupled Weight Decay Regularization}",
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={ICLR},
  year={2019}
}

@inproceedings{NIPS2008_f7664060,
 author = {Hoyer, Patrik and Janzing, Dominik and Mooij, Joris M and Peters, Jonas and Sch\"{o}lkopf, Bernhard},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = "{Nonlinear causal discovery with additive noise models}",
 url = {https://proceedings.neurips.cc/paper/2008/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf},
 volume = {21},
 year = {2009}
}

@InProceedings{EigenHSIC,
author="{Daniu{\v{s}}is, P.
and Vaitkus, P.}",
editor="{Corchado, Emilio
and Yin, Hujun}",
title="{Supervised Feature Extraction Using Hilbert-Schmidt Norms}",
booktitle="{Intelligent Data Engineering and Automated Learning - IDEAL 2009}",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="25--33",
abstract="We propose a novel, supervised feature extraction procedure, based on an unbiased estimator of the Hilbert-Schmidt independence criterion (HSIC). The proposed procedure can be directly applied to single-label or multi-label data, also the kernelized version can be applied to any data type, on which a positive definite kernel function has been defined. Computer experiments with various classification data sets reveal that our approach can be applied more efficiently than the alternative ones.",
isbn="978-3-642-04394-9"
}

@book{10.5555/3279302,
author = {Sch\"{o}lkopf, Bernhard and Smola, Alexander J. and Bach, Francis},
title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
year = {2018},
isbn = {0262536579},
publisher = {The MIT Press},
abstract = {A comprehensive introduction to Support Vector Machines and related kernel methods. In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernelsfor a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{Ma2020TheHB,
  title={The HSIC Bottleneck: Deep Learning without Back-Propagation},
  author={Kurt Wan-Duo Ma and J. P. Lewis and W. Kleijn},
  journal={ArXiv},
  year={2020},
  volume={abs/1908.01580}
}


@article{WANG2021107567,
	title = {Learning with Hilbert–Schmidt independence criterion: A review and new perspectives},
	journal = {Knowledge-Based Systems},
	volume = {234},
	pages = {107567},
	year = {2021},
	issn = {0950-7051},
	doi = {https://doi.org/10.1016/j.knosys.2021.107567},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705121008297},
	author = {Tinghua Wang and Xiaolu Dai and Yuze Liu},
	keywords = {Hilbert–Schmidt independence criterion (HSIC), Feature selection, Dimensionality reduction, Clustering, Kernel method, Machine learning},
	abstract = {The Hilbert–Schmidt independence criterion (HSIC) was originally designed to measure the statistical dependence of the distribution-based Hilbert space embedding in statistical inference. In recent years, it has been witnessed that this criterion can tackle a large number of learning problems owing to its effectiveness and high efficiency. In this article, we provide an in-depth survey of learning methods using the HSIC for various learning problems, like feature selection, dimensionality reduction, clustering, and kernel learning and optimization. Specifically, after introducing the basic idea of HISC, we systematically review the typical learning models based on the HISC, ranging from supervised learning to unsupervised learning, as well as from traditional machine learning to transfer learning and deep learning, followed by remaining challenges and future directions. The relationships between learning methods using the HSIC and other relevant learning algorithms are also discussed. We expect to provide practitioners valuable guidelines for their specific domains by elucidating the similarities and differences of these learning models.}
}

@inproceedings{
	li2021selfsupervised,
	title={Self-Supervised Learning with Kernel Dependence Maximization},
	author={Yazhe Li and Roman Pogodin and Danica J. Sutherland and Arthur Gretton},
	booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
	year={2021},
	url={https://openreview.net/forum?id=0HW7A5YZjq7}
}

@article{Edlemann,
	author={Dominic Edelmann and Konstantinos Fokianos and Maria Pitsillou},
	title={{An Updated Literature Review of Distance Correlation and Its Applications to Time Series}},
	journal={International Statistical Review},
	year=2019,
	volume={87},
	number={2},
	pages={237-262},
	month={August},
	keywords={},
	doi={10.1111/insr.12294},
	abstract={The concept of distance covariance/correlation was introduced recently to characterise dependence among vectors of random variables. We review some statistical aspects of distance covariance/correlation function, and we demonstrate its applicability to time series analysis. We will see that the auto‐distance covariance/correlation function is able to identify non‐linear relationships and can be employed for testing the i.i.d. hypothesis. Comparisons with other measures of dependence are included.},
	url={https://ideas.repec.org/a/bla/istatr/v87y2019i2p237-262.html}
}


@article{OpenML2013,
	author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
	title = {OpenML: Networked Science in Machine Learning},
	journal = {SIGKDD Explorations},
	volume = {15},
	number = {2},
	year = {2013},
	pages = {49--60},
	url = {http://doi.acm.org/10.1145/2641190.2641198},
	doi = {10.1145/2641190.2641198},
	publisher = {ACM},
	address = {New York, NY, USA},
}

@book{Jacod,
	author = {Jacod Jean},
	address = {Berlin Heidelberg New York},
	booktitle = {Probability essentials},
	edition = {2nd edition},
	isbn = {978-3-540-43871-7},
	keywords = {Probabilités},
	language = {eng},
	publisher = {Springer},
	series = {Universitext},
	title = {Probability essentials / Jean Jacod, Philip Protter},
	year = {2003},
}

@article{hsca, title="{Hilbert–Schmidt component analysis}", volume={57}, url={https://www.journals.vu.lt/LMR/article/view/14932}, DOI={10.15388/LMR.A.2016.02}, abstractNote={&lt;p align=&quot;LEFT&quot;&gt;We propose a feature extraction algorithm, based on the Hilbert–Schmidt independence criterion (HSIC) and the maximum dependence – minimum redundancy approach. Experiments with classification data sets demonstrate that suggested Hilbert–Schmidt component analysis (HSCA) algorithm in certain cases may be more efficient than other considered approaches.&lt;/p&gt; &lt;p&gt;&amp;nbsp;&lt;/p&#38;gt;}, number={A}, journal={Lithuanian mathematical journal}, author={Daniušis, P. and Vaitkus, Pr., and Petkevičius, L.}, year={2016}, month={Dec.}, pages={7–11} }

@incollection{NIPS2019_9147,
	title = "{Sobolev Independence Criterion}",
	author = "{Mroueh, Youssef and Sercu, Tom and Rigotti, Mattia and Padhi, Inkit and Nogueira dos Santos, Cicero}",
	booktitle = "{Advances in Neural Information Processing Systems 32},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}",
	pages = {9505--9515},
	year = {2019},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/9147-sobolev-independence-criterion.pdf}
}

@inproceedings{NIPS2004_42fe8808,
	author = {Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam and Salakhutdinov, Russ R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {L. Saul and Y. Weiss and L. Bottou},
	pages = {},
	publisher = {MIT Press},
	title = "{Neighbourhood Components Analysis}",
	url = {https://proceedings.neurips.cc/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf},
	volume = {17},
	year = {2004}
}

@article{Ragonesi2021LearningUR,
	title="{Learning Unbiased Representations via Mutual Information Backpropagation}",
	author={Ruggero Ragonesi and Riccardo Volpi and Jacopo Cavazza and Vittorio Murino},
	journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
	year={2021},
	pages={2723-2732}
}


@InProceedings{pmlr-v80-belghazi18a,
	title = 	 "{Mutual Information Neural Estimation}",
	author =       {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {531--540},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {10--15 Jul},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v80/belghazi18a/belghazi18a.pdf},
	url = 	 {https://proceedings.mlr.press/v80/belghazi18a.html},
	abstract = 	 {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.}
}

@Inbook{Wilcoxon1992,
	author="{Wilcoxon, Frank}",
	editor="Kotz, Samuel
	and Johnson, Norman L.",
	title="{Individual Comparisons by Ranking Methods}",
	bookTitle="Breakthroughs in Statistics: Methodology and Distribution",
	year="1992",
	publisher="Springer New York",
	address="New York, NY",
	pages="196--202",
	abstract="The comparison of two treatments generally falls into one of the following two categories: (a) we may have a number of replications for each of the two treatments, which are unpaired, or (b) we may have a number of paired comparisons leading to a series of differences, some of which may be positive and some negative. The appropriate methods for testing the significance of the differences of the means in these two cases are described in most of the textbooks on statistical methods.",
	isbn="978-1-4612-4380-9",
	doi="10.1007/978-1-4612-4380-9_16",
	url="https://doi.org/10.1007/978-1-4612-4380-9_16"
}

@article{10.1145/1839490.1839495,
	author = {Zhang, Yin and Zhou, Zhi-Hua},
	title = {Multilabel Dimensionality Reduction via Dependence Maximization},
	year = {2010},
	issue_date = {October 2010},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {4},
	number = {3},
	issn = {1556-4681},
	url = {https://doi.org/10.1145/1839490.1839495},
	doi = {10.1145/1839490.1839495},
	abstract = {Multilabel learning deals with data associated with multiple labels simultaneously. Like other data mining and machine learning tasks, multilabel learning also suffers from the curse of dimensionality. Dimensionality reduction has been studied for many years, however, multilabel dimensionality reduction remains almost untouched. In this article, we propose a multilabel dimensionality reduction method, MDDM, with two kinds of projection strategies, attempting to project the original data into a lower-dimensional feature space maximizing the dependence between the original feature description and the associated class labels. Based on the Hilbert-Schmidt Independence Criterion, we derive a eigen-decomposition problem which enables the dimensionality reduction process to be efficient. Experiments validate the performance of MDDM.},
	journal = {ACM Trans. Knowl. Discov. Data},
	month = {oct},
	articleno = {14},
	numpages = {21},
	keywords = {Dimensionality reduction, multilabel learning}
}


