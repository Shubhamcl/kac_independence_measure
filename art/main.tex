\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{blindtext}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}

\title{Statistical dependence measure based on characteristic functions}
\author{povilas.daniusis, povilasd@neurotechnology.com}
\date{November 2021}

\begin{document}
\maketitle

\begin{abstract}
    In this paper we propose  multivariate statistical dependence measure based on the difference between joint and product of marginal characteristic functions. We discuss simulated examples, and applications for feature selection/extraction, causal inference and conduct corresponding experiments with diverse collection of multivariate data sets.
\end{abstract}

\section{Introduction}
Estimation of statistical dependence, both qualitatively and quantitatively, plays important role in various statistical and machine learning methods (e.g. hypothesis testing, feature selection and extraction~\cite{?}, information bottleneck methods \cite{?}, cost function / reinforcement learning reward design, causal inference~\cite{?}, among others). Therefore, earliest statistical dependence estimation ideas (e.g. conditional probability) likely share nearly-common origin with the beginning of formal statistical reasoning itself. During last two centuries ideas of correlation and (relative) entropy (including various generalizations) were proposed and became very popular in numerous applications and theoretical developments. However, with the increasing popularity of statistical machine learning, new statistical dependence estimation methods, that are robust, applicable to noisy, high-dimensional data, and can be efficiently integrated with modern machine learning methods are helpful for the development both of the theory and application.

In this study we will begin with the short review of some important previous dependence estimation approaches (Section~\ref{section:previous_work}), devoting special attention to ones based on characteristic functions (Section~\ref{section:previous_work_cf}). Afterwards we formulate new characteristic function-based statistical dependence measure and its empirical estimator (Section~\ref{section:proposed_method}), which is the main theoretical contribution of our paper. Section~\ref{section:experiments} is devoted to experiments with simulated and real data sets, and finalizing Section~\ref{section:discussion} concludes this article.

\section{Previous work}
\label{section:previous_work}
Shannon mutual information \cite{?} and generalizations \cite{?},  Hilbert-Schmidt independence criterion \cite{?}, distance correlation\cite{?}. Survey \cite{?}.

\subsection{Characteristic-function-based methods}
\label{section:previous_work_cf}
\cite{Feuerverger} (...) 
~\cite{Bottcher} proposes dependence measures for $n$ random variables of arbitrary dimesnions \textit{distance multivariance} and derive from it another dependence measure, called  \textit{total distance multivariance}. Both measure rely (...)
\cite{Belu2012MultivariateMO}
\section{Proposed Method}
\label{section:proposed_method}
Characteristic function of random vector $X$ is defined as 
\begin{equation}
    \label{eq:characteristic_function}
    \phi_{X}(\alpha) = \mathbb{E_{X}} e^{i\alpha^{T}X}, 
\end{equation}
where $i=\sqrt{-1}$.

We will derive independence measure relying on property of characteristic functions (also known as Kac theorem~\cite{KacTheorem}), that independence of two random vectors $X \in R^{d_{x}}$ and $Y \in R^{d_{y}}$ is equivalent to $\forall \alpha \in R^{d_x}, \beta \in R^{d_y} $, \begin{equation}
%\mathbb{E}_{X,Y} e^{i <\alpha, X> + i <\beta, Y>} = \mathbb{E}_{X} e^{i <\alpha, X>} \mathbb{E}_{Y} e^{i <\beta, Y>},
\phi_{X,Y}(\alpha + \beta) = \phi_{X}(\alpha) \phi_{Y}(\beta),
\end{equation}
where $d_{y}$ are dimensions of $X$ and $Y$, respectively.

\noindent This motivates the construction of a novel statistical measure, which we further refer to as Kac independence measure (KacIM):
\begin{equation}
\label{eq:kim}
%    \kappa(X,Y) = \max_{\alpha, \beta} \vert \mathbb{E}_{X,Y} e^{i <\alpha, X> + i <\beta, Y>} -\mathbb{E}_{X} e^{i <\alpha, X>} \mathbb{E}_{Y} e^{i <\beta, Y>} \vert
    \kappa(X,Y) = \max_{\alpha, \beta} \vert \phi_{X,Y}(\alpha + \beta)  -\phi_{X}(\alpha) \phi_{Y}(\beta) \vert
\end{equation}

\noindent It is easy to see that $0 \leq \kappa(X,Y) \leq 1$, $\kappa(X,Y) = \kappa(Y,X)$. 

\subsection{Estimation}

Having i.i.d. data $(x_{j}, y_{j})$, $j = 1,2,...,n$ an empirical estimator of KacIM~\eqref{eq:kim} is defined via corresponding empirical characteristic functions:
\begin{equation}
\label{eq:estimator}
    \hat{\kappa}(X,Y) = \max_{\|\alpha\| = \|\beta\| = 1} \vert \frac{1}{n} \sum_{j=1}^{n} e^{i(<\alpha, x_{j}> + <\beta, y_{j}>) } - \frac{1}{n^2} \sum_{j=1}^{n} e^{i <\alpha, x_{j}>}\sum_{k=1}^{n} e^{i<\beta, y_{k}>}\vert.
\end{equation}

\noindent Empirical estimator also admits $0 \leq \hat{\kappa}(X,Y) \leq 1$. Normalisation of parameters $\alpha$ and $\beta$ on to unit sphere is included due to stability issues. The  estimator~\eqref{eq:estimator} can be calculated by using Algorithm~\ref{alg:estimator_computation}. Pytorch~\cite{Pytorch} implementation of corresponding procedure can be accessed from \url{https://github.com/povidanius/kac_independence_measure}.

\begin{algorithm}
\caption{KacIM estimator computation algorithm}\label{alg:estimator_computation}
\begin{algorithmic}
\Require data batch $(x,y)$, gradient-based optimiser $GradOpt(loss)$
%\Ensure $y = x^n$
\State Calculate KacIM estimator $\hat{\kappa}(x,y)$, without maximization step (i.e. using current $\alpha, \beta$).
\State Perform one maximization iteration of computed $\hat{\kappa}(x,y)$ via $\alpha, \beta \rightarrow GradOpt(\hat{\kappa}(x,y))$.
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\label{section:experiments}
Dependence measures have board area of applications. For example, regularization~\cite{?,?}, feature selection and extraction~\cite{?}, information bottleneck methods \cite{?}, causal inference~\cite{?}, among others. Further we will conduct empirical investigation of KacIM. Starting with simple illustrative simulations, we will reformulate some key ideas in aforementioned topics for KacIM, and experimentally investigate corresponding empirical scenarios.


\subsection{Generated data}

We begin with simple example, which demonstrates the efficiency of KacIM for simulated multivariate data with additive and multiplicative noise.

\begin{figure}[t]
\label{fig:experiments_simulation}
\centering
\includegraphics[scale=0.25]{./out.png}
\caption{Dependence detection in additive (left) and multiplicative (right) noise scenario.}
\end{figure}

In Figure~\ref{fig:experiments_simulation} reflects KacIM values during iterative adaptation ($500$ iterations). In the case of independent data, both $x_{i}$ and $y_{i}$ ($d_{x} = 1024$, $d_{y} = 32$) are sampled from gaussian distribution, independently. In the case of dependent data, an additive noise (left graph) and multiplicative noise (right graph), the dependent variable is generated according to $y_{i} = sin(P x_{i}) + cos(P x_{i}) + \lambda \epsilon_{i}$ ($\lambda = 0.15$) and $y_{i} = (sin(P x_{i}) + cos(P x_{i})) \epsilon_{i}$, respectively, where $P$ is $d_{x} \times d_{y}$ random projection matrix, $\epsilon_{i} \sim N(0,1)$ and $\epsilon_{i} \perp x_{i}$.

When data is independent (blue graph), both in additive and multiplicative cases, due to independence, estimator~\eqref{eq:estimator} is resistant to maximization, and oscillates near zero. On the other hand, when data is not independent (orange graph), the condition of Kac theorem is violated and maximization of estimator~\eqref{eq:estimator} is possible.

\subsection{Influence of noise level $\lambda$ to KacIM estimator value}
In this simulation we use the same additive noise setting as in previous paragraph, but evaluate all noise levels $\lambda \in [0.0, 2.4]$, with step $0.1$.
Figure~\ref{fig:experiments_noise_level_effect} empirically shows that value of KacIM correlates with noise level, and therefore the proposed measure is able not only to detect whether independence is present, but also to quantitatively evaluate it, which enables to use KacIM to derive cost functions for vairous other learning-based algorithms. %This continuity property is useful for various applications (e.g. using KacIM as the basis of cost function in various algorithms).

\begin{figure}[t]
\label{fig:experiments_noise_level_effect}
\centering
\includegraphics[scale=0.50]{./noise_level_effect_to_kacim.png}
\caption{Noise level ($x$ axis) vs final iteration KacIM value ($y$ axis). KacIM values for larger noise levels saturates as in tail of graph.}
\end{figure}



\subsection{Feature Extraction}

We conduct linear feature extraction by seeking 

\begin{equation}
\label{eq:kim_feature_extraction}    
W^{*} = arg \max_{W} \kappa(Wx, y).
\end{equation}


%\begin{equation}
%\label{eq:kim_feature_extraction1}    
%w_{t}^{*} = arg \max_{W} \frac{\kappa(w_{t}^{T}x, y)}{\kappa(w_{t}^{T}x, W_{t}x)}.
%\end{equation}

\noindent Afterwards, feature extraction is conducted by $f = W^{*}x$ and $k$-nearest neighbor classification with Euclidean distance is performed, comparing unmodified inputs $x$ and features of all possible dimensions up to $d_{x}$.

\section{Discussion} 

\label{section:discussion}
In this article we propose statistical dependence measure, KacIM, which relies on simple fact that statistical independence is equivalent to the decomposability of joint characteristic function  into the product of marginal ones. Although we formulated and analysed KacIM for bivariate vectorial case, similarly it can be generalised for multivariate case. In addition, since characteristic functions are defined for matrices, graphs, and other objects \cite{?}, likely KacIM can be extended to those objects as well, which is potential direction of future research of KacIM.

Empirical analysis show, that KacIM can detect and measure statistical independence for non-linearly related, high-dimensional data. (...)

\section{Acknowledgements}


%\subsection{KacIM for information bottleneck}
%\subsection{Canonical component analysis, independent component analysis}
%\subsection{Causal inference}
%\subsection{Electroencephalography (?)}
%\section{Notes}
%Compare with  mutual information.


%\bibliographystyle{apalike}
\bibliographystyle{unsrt}

{\footnotesize
\bibliography{bibliography}}

% https://arxiv.org/pdf/2104.06612.pdf

%\begin{thebibliography}{}
%\bibitem{KacTheorem} David Applebaum, B.V. Rajarama Bhat, Johan Kustermans, J. Martin Lindsay, Michael Schuermann, Uwe Franz: Quantum Independent Increment Processes I: From Classical Probability to Quantum Stochastic Calculus
%\end{thebibliography}

\end{document}
