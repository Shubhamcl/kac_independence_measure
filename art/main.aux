\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Gretton2005MeasuringSD}
\citation{EigenHSIC}
\citation{HSCA}
\citation{Ma2020TheHB}
\citation{NIPS2008_f7664060}
\citation{li2021selfsupervised}
\citation{Ragonesi2021LearningUR}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Cover2006}
\citation{Gretton2005MeasuringSD}
\citation{Feuerverger}
\citation{Szekely}
\citation{Pczos2012CopulabasedKD}
\citation{NIPS2019_9147}
\citation{LCT}
\@writefile{toc}{\contentsline {section}{\numberline {2}Previous Work}{2}{section.2}}
\newlabel{section:previous_work}{{2}{2}{Previous Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Characteristic-function-based methods}{2}{subsection.2.1}}
\newlabel{section:previous_work_cf}{{2.1}{2}{Characteristic-function-based methods}{subsection.2.1}{}}
\newlabel{eq:characteristic_function}{{1}{2}{Characteristic-function-based methods}{equation.2.1}{}}
\newlabel{eq:ecf}{{2}{2}{Characteristic-function-based methods}{equation.2.2}{}}
\newlabel{eq:joint_characteristic_function}{{3}{2}{Characteristic-function-based methods}{equation.2.3}{}}
\newlabel{eq:joint_ecf}{{4}{2}{Characteristic-function-based methods}{equation.2.4}{}}
\citation{Feuerverger}
\citation{Szekely}
\citation{Szekely}
\citation{Bottcher}
\citation{Szekely}
\citation{CHAUDHURI201915}
\citation{Szekely}
\citation{Szekely}
\citation{Edlemann}
\citation{NEURIPS2019_9015}
\newlabel{eq:kac_theorem}{{5}{3}{Characteristic-function-based methods}{equation.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Proposed Independence Measure}{3}{section.3}}
\newlabel{section:proposed_method}{{3}{3}{Proposed Independence Measure}{section.3}{}}
\newlabel{eq:kim}{{6}{3}{Proposed Independence Measure}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Basic Properties}{3}{subsection.3.1}}
\newlabel{thm:properties}{{1}{3}{}{theorem.1}{}}
\citation{Jacod}
\citation{KacTheorem}
\citation{NEURIPS2019_9015}
\citation{Loshchilov2019DecoupledWD}
\citation{Cover2006}
\citation{pmlr-v80-belghazi18a}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Estimation}{4}{subsection.3.2}}
\newlabel{eq:estimator}{{7}{4}{Estimation}{equation.3.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces KacIM estimation\relax }}{4}{algorithm.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:estimator_computation}{{1}{4}{KacIM estimation\relax }{algorithm.1}{}}
\citation{10.5555/3279302}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Interpretation and connection previous approaches}{5}{subsection.3.3}}
\newlabel{eq:gaussian_kacim}{{9}{5}{Interpretation and connection previous approaches}{equation.3.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\newlabel{section:experiments}{{4}{5}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Generated data}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {paragraph}{Non-linear statistical dependence detection.}{5}{section*.1}}
\citation{Szekely}
\citation{EigenHSIC}
\citation{HSCA}
\citation{10.1145/1839490.1839495}
\citation{OpenML2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Left figure: KacIM evaluation for independent data (blue), additive (orange) and multiplicative (green) noise scenarios ($x$ axis - iteration, and $y$ - corresponding value of KacIM). Right figure: noise level ($x$ axis) vs final iteration KacIM value ($y$ axis). KacIM values for larger noise levels saturates as in tail of graph\relax }}{6}{figure.caption.2}}
\newlabel{fig:experiments_simulation}{{1}{6}{Left figure: KacIM evaluation for independent data (blue), additive (orange) and multiplicative (green) noise scenarios ($x$ axis - iteration, and $y$ - corresponding value of KacIM). Right figure: noise level ($x$ axis) vs final iteration KacIM value ($y$ axis). KacIM values for larger noise levels saturates as in tail of graph\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Noise variance effect}{6}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{Comparison with distance correlation}{6}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Feature extraction}{6}{subsection.4.2}}
\citation{NIPS2004_42fe8808}
\citation{Wilcoxon1992}
\citation{Wilcoxon1992}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The dimension of data is on the $x$ axis, and on $y$ axis is evaluation of distance correlation (left) and KacIM (right). Blue graph corresponds of independent data of dimension, indicated by $x$ axis, and orange one corresponds to dependent data.\relax }}{7}{figure.caption.5}}
\newlabel{fig:experiments_simulation_dcor}{{2}{7}{The dimension of data is on the $x$ axis, and on $y$ axis is evaluation of distance correlation (left) and KacIM (right). Blue graph corresponds of independent data of dimension, indicated by $x$ axis, and orange one corresponds to dependent data.\relax }{figure.caption.5}{}}
\newlabel{eq:kim_feature_extraction}{{10}{7}{Feature extraction}{equation.4.10}{}}
\citation{Loshchilov2019DecoupledWD}
\citation{Wilcoxon1992}
\citation{Wilcoxon1992}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Classification accuracies. $N$ denotes full data set size, $d_{x}$ - input dimensionality, and $n_{c}$ - number of classes. In this table feature dimension is equal to a half of original input dimension. Best accuracies that are also statistically significant (Wilcoxon's signed rank test\nobreakspace  {}\cite  {Wilcoxon1992}, 25 runs, $p$-value threshold $0.01$) are indicated in bold text.\relax }}{8}{table.caption.6}}
\newlabel{table:classification_accuracies}{{1}{8}{Classification accuracies. $N$ denotes full data set size, $d_{x}$ - input dimensionality, and $n_{c}$ - number of classes. In this table feature dimension is equal to a half of original input dimension. Best accuracies that are also statistically significant (Wilcoxon's signed rank test~\cite {Wilcoxon1992}, 25 runs, $p$-value threshold $0.01$) are indicated in bold text.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Regularisation}{8}{subsection.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Top figure - benign moles, bottom figure - maligant tumors.\relax }}{9}{figure.caption.7}}
\newlabel{fig:Pneumonia_dataset_examples}{{3}{9}{Top figure - benign moles, bottom figure - maligant tumors.\relax }{figure.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Classification accuracy comparison of regularised and not regularised model. Bold text indicates that model with regulariser was more accurate (Wilcoxon's signed rank test\nobreakspace  {}\cite  {Wilcoxon1992}, 30 runs, $p$-value threshold $0.04$))\relax }}{9}{table.caption.8}}
\newlabel{table:regularisation_classification_accuracies}{{2}{9}{Classification accuracy comparison of regularised and not regularised model. Bold text indicates that model with regulariser was more accurate (Wilcoxon's signed rank test~\cite {Wilcoxon1992}, 30 runs, $p$-value threshold $0.04$))\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{9}{section.5}}
\newlabel{section:conclusion}{{5}{9}{Conclusion}{section.5}{}}
\bibstyle{unsrt}
\bibdata{bibliography}
\bibcite{Gretton2005MeasuringSD}{1}
\bibcite{EigenHSIC}{2}
\bibcite{HSCA}{3}
\bibcite{Ma2020TheHB}{4}
\bibcite{NIPS2008_f7664060}{5}
\bibcite{li2021selfsupervised}{6}
\bibcite{Ragonesi2021LearningUR}{7}
\bibcite{Cover2006}{8}
\bibcite{Feuerverger}{9}
\bibcite{Szekely}{10}
\bibcite{Pczos2012CopulabasedKD}{11}
\bibcite{NIPS2019_9147}{12}
\bibcite{Bottcher}{13}
\bibcite{CHAUDHURI201915}{14}
\@writefile{toc}{\contentsline {section}{\numberline {6}Acknowledgements}{10}{section.6}}
\bibcite{Edlemann}{15}
\bibcite{NEURIPS2019_9015}{16}
\bibcite{Jacod}{17}
\bibcite{KacTheorem}{18}
\bibcite{Loshchilov2019DecoupledWD}{19}
\bibcite{pmlr-v80-belghazi18a}{20}
\bibcite{10.5555/3279302}{21}
\bibcite{10.1145/1839490.1839495}{22}
\bibcite{OpenML2013}{23}
\bibcite{NIPS2004_42fe8808}{24}
\bibcite{Wilcoxon1992}{25}
